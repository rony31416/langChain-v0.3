{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f045f299",
   "metadata": {},
   "source": [
    "# Introduction to AI Agents with Ollama and Llama3.2\n",
    "\n",
    "## Introduction to Tools\n",
    "\n",
    "Tools are a way to augment our LLMs with code execution. A tool is simply a function formatted so that our agent can understand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
    "\n",
    "We can use the `@tool` decorator to create an LLM-compatible tool from a standard Python function â€” this function should include a few things for optimal performance:\n",
    "\n",
    "- A docstring describing what the tool does and when it should be used. This will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
    "- Clear parameter names that ideally tell the LLM what each parameter is. If it isn't clear, we make sure the docstring explains what the parameter is for and how to use it.\n",
    "- Both parameter and return type annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d3a57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the necessary packages\n",
    "!pip install -q langchain langchain_community requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ae4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",  # Model name\n",
    "    temperature=0.0,    # Lower temperature for more deterministic outputs\n",
    "    base_url=\"http://localhost:11434\"  # Default Ollama server URL\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Hello, are you working?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5ef3b",
   "metadata": {},
   "source": [
    "## Creating Tools\n",
    "\n",
    "With Ollama, we can still use the `@tool` decorator to create tools, but we need to handle tool calling differently since Ollama doesn't support the `bind_tools` method directly. Let's create our mathematical tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "208f59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b020a",
   "metadata": {},
   "source": [
    "\n",
    "With the `@tool` decorator, our function is turned into a `StructuredTool` object, which we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3744d7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name='add'\n",
      "add.description=\"Add 'x' and 'y'.\"\n",
      "{'description': \"Add 'x' and 'y'.\", 'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add', 'type': 'object'}\n",
      "{'description': \"Raise 'x' to the power of 'y'.\", 'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'exponentiate', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name=}\\n{add.description=}\")\n",
    "print(add.args_schema.model_json_schema())\n",
    "print(exponentiate.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c0005",
   "metadata": {},
   "source": [
    "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bb83460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 5, 'y': 2}\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "print(llm_output_dict)\n",
    "\n",
    "# This is then passed into the tool function as kwargs (keyword arguments) as indicated by the ** operator\n",
    "result = exponentiate.func(**llm_output_dict)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb695cb",
   "metadata": {},
   "source": [
    "## Creating a Custom Agent for Ollama\n",
    "\n",
    "Since Ollama doesn't support the `bind_tools` method required by LangChain's `create_tool_calling_agent`, we need to create a custom agent implementation. We'll implement a simple tool-calling loop that works with Ollama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1afb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_tool_agent(llm, tools, user_input, max_steps: int = 6):\n",
    "    \"\"\"\n",
    "    Simple tool-calling loop for LLMs that don't implement bind_tools().\n",
    "    Expects the LLM to either:\n",
    "      - return a JSON object like {\"tool\": \"add\", \"x\": 2, \"y\": 3}\n",
    "      - or produce a non-JSON final answer (treated as the final response)\n",
    "    \"\"\"\n",
    "    import json, textwrap\n",
    "    tools_map = {t.name: t for t in tools}  # StructuredTool objects from the notebook\n",
    "    scratchpad = []  # keep a running history of tool calls / observations\n",
    "    for step in range(max_steps):\n",
    "        # Build a short instruction + scratchpad to send to the LLM\n",
    "        instruction = (\n",
    "            \"You are a calculator agent. When you need to use a tool, output a JSON object \"\n",
    "            \"with the key 'tool' and parameters for that tool, e.g. \"\n",
    "            \"{\\\"tool\\\": \\\"add\\\", \\\"x\\\": 2, \\\"y\\\": 3}. \"\n",
    "            \"When you are done, return the final answer as plain text.\"\n",
    "        )\n",
    "        if scratchpad:\n",
    "            instruction += \"\\n\\nScratchpad:\\n\" + \"\\n\".join(scratchpad)\n",
    "        prompt_text = f\"{instruction}\\n\\nUser: {user_input}\\n\\nAgent:\"\n",
    "        resp = llm.invoke(prompt_text).strip()\n",
    "        # Try to parse JSON -> treat as tool call\n",
    "        try:\n",
    "            payload = json.loads(resp)\n",
    "            if not isinstance(payload, dict) or \"tool\" not in payload:\n",
    "                # Not the expected shape -> treat as final answer\n",
    "                return resp\n",
    "            tool_name = payload.pop(\"tool\")\n",
    "            if tool_name not in tools_map:\n",
    "                scratchpad.append(f\"ERROR: unknown tool '{tool_name}'\")\n",
    "                continue\n",
    "            tool = tools_map[tool_name]\n",
    "            # call the underlying function (StructuredTool stores original func on .func)\n",
    "            observation = tool.func(**payload)\n",
    "            scratchpad.append(f\"CALL {tool_name} -> {observation}\")\n",
    "            # continue the loop so the LLM can use the observation\n",
    "            continue\n",
    "        except json.JSONDecodeError:\n",
    "            # Not JSON => assume final natural language answer\n",
    "            return resp\n",
    "    # If max_steps exhausted, return the last LLM response or a timeout message\n",
    "    return \"Agent stopped after max steps. \" + (resp if 'resp' in locals() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751ce73",
   "metadata": {},
   "source": [
    "Now let's test our custom agent with a mathematical query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79ad17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tool\": \"multiply\", \"x\": \"(2+3)\", \"y\": 4}\n",
      "{\"tool\": \"add\", \"x\": 5, \"y\": 0}\n",
      "{\"tool\": \"power\", \"x\": 5, \"y\": 2} \n",
      "\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Define our tools\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "# Example usage\n",
    "user_input = \"Compute (2+3)*4 and then raise the result to the power 2.\"\n",
    "result = run_simple_tool_agent(llm=llm, tools=tools, user_input=user_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb90f7c",
   "metadata": {},
   "source": [
    "## Adding Memory to Our Agent\n",
    "\n",
    "To make our agent remember previous interactions, we need to add memory functionality. Let's create an enhanced version of our agent that includes conversation memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a13c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tool_agent_with_memory(llm, tools, user_input, memory=None, max_steps: int = 6):\n",
    "    \"\"\"\n",
    "    Tool-calling agent with memory for LLMs that don't implement bind_tools().\n",
    "    \"\"\"\n",
    "    import json, textwrap\n",
    "    tools_map = {t.name: t for t in tools}\n",
    "    scratchpad = []\n",
    "    \n",
    "    # Format conversation history\n",
    "    history = \"\"\n",
    "    if memory is not None:\n",
    "        for message in memory.chat_memory.messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                history += f\"Human: {message.content}\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                history += f\"Assistant: {message.content}\\n\"\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        instruction = (\n",
    "            \"You are a helpful assistant. You have access to the following tools: \" + \n",
    "            \", \".join(tools_map.keys()) + \".\\n\\n\" +\n",
    "            \"When you need to use a tool, output a JSON object with the key 'tool' and parameters for that tool, e.g. \" +\n",
    "            \"{\\\"tool\\\": \\\"add\\\", \\\"x\\\": 2, \\\"y\\\": 3}.\\n\\n\" +\n",
    "            \"When you are done, return the final answer as plain text.\\n\\n\" +\n",
    "            \"Conversation History:\\n\" + history + \"\\n\\n\" +\n",
    "            \"Scratchpad:\\n\" + \"\\n\".join(scratchpad) + \"\\n\\n\" +\n",
    "            \"User: \" + user_input + \"\\n\\n\" +\n",
    "            \"Agent:\"\n",
    "        )\n",
    "        resp = llm.invoke(instruction).strip()\n",
    "        # Try to parse JSON -> treat as tool call\n",
    "        try:\n",
    "            payload = json.loads(resp)\n",
    "            if not isinstance(payload, dict) or \"tool\" not in payload:\n",
    "                # Not the expected shape -> treat as final answer\n",
    "                # Update memory with the final answer\n",
    "                if memory is not None:\n",
    "                    memory.save_context({\"input\": user_input}, {\"output\": resp})\n",
    "                return resp\n",
    "            tool_name = payload.pop(\"tool\")\n",
    "            if tool_name not in tools_map:\n",
    "                scratchpad.append(f\"ERROR: unknown tool '{tool_name}'\")\n",
    "                continue\n",
    "            tool = tools_map[tool_name]\n",
    "            observation = tool.func(**payload)\n",
    "            scratchpad.append(f\"CALL {tool_name} -> {observation}\")\n",
    "            # continue the loop\n",
    "        except json.JSONDecodeError:\n",
    "            # Not JSON => assume final natural language answer\n",
    "            if memory is not None:\n",
    "                memory.save_context({\"input\": user_input}, {\"output\": resp})\n",
    "            return resp\n",
    "    # If max_steps exhausted, return the last LLM response or a timeout message\n",
    "    result = \"Agent stopped after max steps. \" + (resp if 'resp' in locals() else \"\")\n",
    "    if memory is not None:\n",
    "        memory.save_context({\"input\": user_input}, {\"output\": result})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88da7",
   "metadata": {},
   "source": [
    "Now let's create a memory object and test our agent with memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "322e2cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent stopped after max steps. {\"tool\": \"multiply\", \"x\": 10.7, \"y\": 7.68}\n",
      "Hello James! It's nice to meet you. How can I assist you today?\n",
      "{\"tool\": \"add\", \"x\": 9, \"y\": 10}\n",
      "{\"tool\": \"subtract\", \"x\": {\"tool\": \"multiply\", \"x\": 4, \"y\": 2}, \"y\": 11}\n",
      "{\"tool\": \"exponentiate\", \"x\": 11, \"y\": 3}\n",
      "Hello James! I'm happy to help you with your questions. You haven't asked a question yet, so let's get started! What would you like to know or calculate?\n"
     ]
    }
   ],
   "source": [
    "# Create a memory object\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
    "    return_messages=True  # to return Message objects\n",
    ")\n",
    "\n",
    "# Test with a simple query\n",
    "result = run_tool_agent_with_memory(\n",
    "    llm=llm, \n",
    "    tools=tools, \n",
    "    user_input=\"what is 10.7 multiplied by 7.68?\",\n",
    "    memory=memory\n",
    ")\n",
    "print(result)\n",
    "\n",
    "# Test with memory\n",
    "result = run_tool_agent_with_memory(\n",
    "    llm=llm, \n",
    "    tools=tools, \n",
    "    user_input=\"My name is James\",\n",
    "    memory=memory\n",
    ")\n",
    "print(result)\n",
    "\n",
    "# Test with a complex calculation\n",
    "result = run_tool_agent_with_memory(\n",
    "    llm=llm, \n",
    "    tools=tools, \n",
    "    user_input=\"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
    "    memory=memory\n",
    ")\n",
    "print(result)\n",
    "\n",
    "# Test if the agent remembers the name\n",
    "result = run_tool_agent_with_memory(\n",
    "    llm=llm, \n",
    "    tools=tools, \n",
    "    user_input=\"What is my name\",\n",
    "    memory=memory\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe840fb",
   "metadata": {},
   "source": [
    "## Creating a Weather Agent\n",
    "\n",
    "Now let's create tools for getting location and weather information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8eb142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_location_from_ip():\n",
    "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://ipinfo.io/json\")\n",
    "        data = response.json()\n",
    "        if 'loc' in data:\n",
    "            latitude, longitude = data['loc'].split(',')\n",
    "            data = (\n",
    "                f\"Latitude: {latitude},\\n\"\n",
    "                f\"Longitude: {longitude},\\n\"\n",
    "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
    "                f\"Country: {data.get('country', 'N/A')}\"\n",
    "            )\n",
    "            return data\n",
    "        else:\n",
    "            return \"Location could not be determined.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Return the current date and time.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a specified location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get weather for, e.g., \"London, UK\"\n",
    "        \n",
    "    Returns:\n",
    "        Weather information as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For this example, we'll simulate weather data\n",
    "        # In a real implementation, you would use a weather API\n",
    "        return f\"The weather in {location} is partly cloudy with a temperature of 22Â°C.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error getting weather: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65012ca1",
   "metadata": {},
   "source": [
    "Now let's test our weather agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f93e87e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"tool\": \"get_current_datetime\",}\n",
       "\n",
       "{\"tool\": \"get_weather\",}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new memory object for the weather agent\n",
    "weather_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Define the weather tools\n",
    "weather_tools = [get_current_datetime, get_location_from_ip, get_weather]\n",
    "\n",
    "# Test the weather agent\n",
    "result = run_tool_agent_with_memory(\n",
    "    llm=llm, \n",
    "    tools=weather_tools, \n",
    "    user_input=\"I have a few questions, what is the date and time right now? How is the weather where I am? Please give me degrees in Celsius\",\n",
    "    memory=weather_memory\n",
    ")\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langChain-v0.3 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
