{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b84fda0",
   "metadata": {},
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc623d",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf86e1",
   "metadata": {},
   "source": [
    "## Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1789fa",
   "metadata": {},
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02518498",
   "metadata": {},
   "source": [
    "### Traditional LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef76310",
   "metadata": {},
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` — the LLM we will be using to generate the output.\n",
    "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0e60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cfccb",
   "metadata": {},
   "source": [
    "For the LLM, we'll start by initializing our connection to the Meta's Ollama ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8b63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",  \n",
    "    temperature=0.0,    \n",
    "    base_url=\"http://localhost:11434\"  # Default Ollama server URL\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec87a061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ceb25f",
   "metadata": {},
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2d94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1362ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35c140",
   "metadata": {},
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371b80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_7648\\2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bba8b",
   "metadata": {},
   "source": [
    "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
    "\n",
    "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75d73b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '**Retrieval-Augmented Generation (RAG)**\\n\\nRetrieval-Augmented Generation (RAG) is a novel approach to natural language processing (NLP) that combines the strengths of both retrieval and generation models. The goal of RAG is to improve the efficiency, accuracy, and coherence of text generation tasks by leveraging pre-trained language models for retrieval.\\n\\n**Key Components:**\\n\\n1. **Retrieval Model:** A pre-trained language model is used as a retrieval component to search for relevant information in a large corpus.\\n2. **Generation Model:** A separate pre-trained language model is used as a generation component to generate text based on the retrieved information.\\n3. **Hybrid Model:** The retrieval and generation models are combined using a hybrid architecture, which allows them to interact with each other during the generation process.\\n\\n**How it Works:**\\n\\n1. **Input Text:** A query or prompt is input into the system.\\n2. **Retrieval:** The retrieval model searches for relevant information in the corpus based on the query.\\n3. **Ranking:** The retrieved documents are ranked based on their relevance to the query.\\n4. **Generation:** The generation model generates text based on the top-ranked document(s) from the retrieval step.\\n\\n**Advantages:**\\n\\n1. **Improved Efficiency:** RAG reduces the computational cost of generating text by leveraging pre-trained models for retrieval.\\n2. **Increased Accuracy:** By using a retrieval component, RAG can improve the accuracy of generated text by selecting relevant information from the corpus.\\n3. **Enhanced Coherence:** The hybrid model allows for more coherent and context-specific generation of text.\\n\\n**Applications:**\\n\\n1. **Text Generation:** RAG can be used for various text generation tasks such as summarization, question answering, and text classification.\\n2. **Conversational AI:** RAG can improve the efficiency and accuracy of conversational AI systems by leveraging pre-trained models for retrieval.\\n3. **Content Generation:** RAG can be used to generate high-quality content such as articles, blog posts, and social media updates.\\n\\n**Conclusion:**\\n\\nRetrieval-Augmented Generation (RAG) is a promising approach to natural language processing that combines the strengths of both retrieval and generation models. By leveraging pre-trained language models for retrieval, RAG can improve the efficiency, accuracy, and coherence of text generation tasks. As the field continues to evolve, we can expect to see more applications of RAG in various domains.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1cd89",
   "metadata": {},
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d870dc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval-Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a novel approach to natural language processing (NLP) that combines the strengths of both retrieval and generation models. The goal of RAG is to improve the efficiency, accuracy, and coherence of text generation tasks by leveraging pre-trained language models for retrieval.\n",
       "\n",
       "**Key Components:**\n",
       "\n",
       "1. **Retrieval Model:** A pre-trained language model is used as a retrieval component to search for relevant information in a large corpus.\n",
       "2. **Generation Model:** A separate pre-trained language model is used as a generation component to generate text based on the retrieved information.\n",
       "3. **Hybrid Model:** The retrieval and generation models are combined using a hybrid architecture, which allows them to interact with each other during the generation process.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "1. **Input Text:** A query or prompt is input into the system.\n",
       "2. **Retrieval:** The retrieval model searches for relevant information in the corpus based on the query.\n",
       "3. **Ranking:** The retrieved documents are ranked based on their relevance to the query.\n",
       "4. **Generation:** The generation model generates text based on the top-ranked document(s) from the retrieval step.\n",
       "\n",
       "**Advantages:**\n",
       "\n",
       "1. **Improved Efficiency:** RAG reduces the computational cost of generating text by leveraging pre-trained models for retrieval.\n",
       "2. **Increased Accuracy:** By using a retrieval component, RAG can improve the accuracy of generated text by selecting relevant information from the corpus.\n",
       "3. **Enhanced Coherence:** The hybrid model allows for more coherent and context-specific generation of text.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "1. **Text Generation:** RAG can be used for various text generation tasks such as summarization, question answering, and text classification.\n",
       "2. **Conversational AI:** RAG can improve the efficiency and accuracy of conversational AI systems by leveraging pre-trained models for retrieval.\n",
       "3. **Content Generation:** RAG can be used to generate high-quality content such as articles, blog posts, and social media updates.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a promising approach to natural language processing that combines the strengths of both retrieval and generation models. By leveraging pre-trained language models for retrieval, RAG can improve the efficiency, accuracy, and coherence of text generation tasks. As the field continues to evolve, we can expect to see more applications of RAG in various domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691aae36",
   "metadata": {},
   "source": [
    "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cab6b2",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebec006",
   "metadata": {},
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f94e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2732f13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Retrieval-Augmented Generation (RAG)**\\n\\nRetrieval-Augmented Generation (RAG) is a novel approach to natural language processing (NLP) that combines the strengths of both retrieval and generation models. The goal of RAG is to leverage the capabilities of retrieval models, which are designed to efficiently search and retrieve relevant information from large databases or knowledge graphs, with those of generation models, which can generate coherent and context-specific text.\\n\\n**Key Components:**\\n\\n1. **Retrieval Model:** A retrieval model is used to search for relevant documents or knowledge graph entities that match the input query.\\n2. **Generation Model:** A generation model is used to generate a response based on the retrieved information.\\n3. **Augmentation Mechanism:** The generated response is then augmented with additional information from the retrieved documents or knowledge graph entities.\\n\\n**How RAG Works:**\\n\\n1. Input Query: A user submits an input query, which can be a natural language sentence or phrase.\\n2. Retrieval Model: The retrieval model searches for relevant documents or knowledge graph entities that match the input query.\\n3. Generation Model: The generation model generates a response based on the retrieved information.\\n4. Augmentation Mechanism: The generated response is augmented with additional information from the retrieved documents or knowledge graph entities.\\n\\n**Advantages of RAG:**\\n\\n1. **Improved Accuracy:** RAG can generate more accurate and coherent responses by leveraging the strengths of both retrieval and generation models.\\n2. **Increased Contextual Understanding:** RAG can provide a better understanding of the context in which the input query is being asked, leading to more relevant and informative responses.\\n3. **Efficient Information Retrieval:** RAG can efficiently retrieve relevant information from large databases or knowledge graphs, reducing the need for manual searching.\\n\\n**Applications of RAG:**\\n\\n1. **Virtual Assistants:** RAG can be used in virtual assistants to provide more accurate and context-specific responses to user queries.\\n2. **Chatbots:** RAG can be used in chatbots to generate more informative and engaging conversations with users.\\n3. **Information Retrieval Systems:** RAG can be used in information retrieval systems to improve the accuracy and relevance of search results.\\n\\n**Conclusion:**\\n\\nRetrieval-Augmented Generation (RAG) is a promising approach to natural language processing that combines the strengths of both retrieval and generation models. By leveraging the capabilities of retrieval models, RAG can generate more accurate and coherent responses while providing a better understanding of the context in which the input query is being asked. The applications of RAG are vast and varied, making it an exciting area of research with significant potential for impact.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3fd6a",
   "metadata": {},
   "source": [
    "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428a46f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval-Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a novel approach to natural language processing (NLP) that combines the strengths of both retrieval and generation models. The goal of RAG is to leverage the capabilities of retrieval models, which are designed to efficiently search and retrieve relevant information from large databases or knowledge graphs, with those of generation models, which can generate coherent and context-specific text.\n",
       "\n",
       "**Key Components:**\n",
       "\n",
       "1. **Retrieval Model:** A retrieval model is used to search for relevant documents or knowledge graph entities that match the input query.\n",
       "2. **Generation Model:** A generation model is used to generate a response based on the retrieved information.\n",
       "3. **Augmentation Mechanism:** The generated response is then augmented with additional information from the retrieved documents or knowledge graph entities.\n",
       "\n",
       "**How RAG Works:**\n",
       "\n",
       "1. Input Query: A user submits an input query, which can be a natural language sentence or phrase.\n",
       "2. Retrieval Model: The retrieval model searches for relevant documents or knowledge graph entities that match the input query.\n",
       "3. Generation Model: The generation model generates a response based on the retrieved information.\n",
       "4. Augmentation Mechanism: The generated response is augmented with additional information from the retrieved documents or knowledge graph entities.\n",
       "\n",
       "**Advantages of RAG:**\n",
       "\n",
       "1. **Improved Accuracy:** RAG can generate more accurate and coherent responses by leveraging the strengths of both retrieval and generation models.\n",
       "2. **Increased Contextual Understanding:** RAG can provide a better understanding of the context in which the input query is being asked, leading to more relevant and informative responses.\n",
       "3. **Efficient Information Retrieval:** RAG can efficiently retrieve relevant information from large databases or knowledge graphs, reducing the need for manual searching.\n",
       "\n",
       "**Applications of RAG:**\n",
       "\n",
       "1. **Virtual Assistants:** RAG can be used in virtual assistants to provide more accurate and context-specific responses to user queries.\n",
       "2. **Chatbots:** RAG can be used in chatbots to generate more informative and engaging conversations with users.\n",
       "3. **Information Retrieval Systems:** RAG can be used in information retrieval systems to improve the accuracy and relevance of search results.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a promising approach to natural language processing that combines the strengths of both retrieval and generation models. By leveraging the capabilities of retrieval models, RAG can generate more accurate and coherent responses while providing a better understanding of the context in which the input query is being asked. The applications of RAG are vast and varied, making it an exciting area of research with significant potential for impact."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21683c37",
   "metadata": {},
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2ae58",
   "metadata": {},
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac5352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0e7d2",
   "metadata": {},
   "source": [
    "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c0ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70275922",
   "metadata": {},
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97a3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df41c90",
   "metadata": {},
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c4b0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22337942",
   "metadata": {},
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d27bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df18520",
   "metadata": {},
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c54ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663a27f",
   "metadata": {},
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e66ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea613c",
   "metadata": {},
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74a7ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d853c",
   "metadata": {},
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a567ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e07fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62489670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Artificial Intelligence (AI) Report**\n",
       "\n",
       "**Introduction:**\n",
       "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. The field of AI has made significant progress in recent years, with applications in various industries, including healthcare, finance, transportation, and education.\n",
       "\n",
       "**Key Developments:**\n",
       "\n",
       "1. **Deep Learning:** A subset of machine learning, deep learning uses neural networks to analyze data and make predictions. This technology has led to breakthroughs in image recognition, natural language processing, and speech recognition.\n",
       "2. **Natural Language Processing (NLP):** NLP enables computers to understand and generate human-like language. This technology is used in virtual assistants, chatbots, and language translation software.\n",
       "3. **Robotics:** AI-powered robots are being used in manufacturing, logistics, and healthcare to perform tasks that require precision and dexterity.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "1. **Healthcare:** AI is being used to analyze medical images, diagnose diseases, and develop personalized treatment plans.\n",
       "2. **Finance:** AI-powered systems are being used to detect fraud, predict stock prices, and optimize investment portfolios.\n",
       "3. **Transportation:** Self-driving cars and trucks are being developed using AI algorithms that enable vehicles to navigate complex roads and make decisions in real-time.\n",
       "\n",
       "**Challenges:**\n",
       "\n",
       "1. **Bias and Fairness:** AI systems can perpetuate biases present in the data used to train them, leading to unfair outcomes.\n",
       "2. **Job Displacement:** The increasing use of AI in industries may lead to job displacement for certain workers.\n",
       "3. **Security:** AI-powered systems can be vulnerable to cyber attacks and data breaches.\n",
       "\n",
       "**Conclusion:**\n",
       "Artificial intelligence has made significant progress in recent years, with applications in various industries. However, there are also challenges associated with the development and deployment of AI systems, including bias, fairness, job displacement, and security concerns. As AI continues to evolve, it is essential to address these challenges and ensure that the benefits of AI are shared by all.\n",
       "\n",
       "**Future Outlook:**\n",
       "The future of AI holds much promise, with potential applications in areas such as:\n",
       "\n",
       "1. **Autonomous Systems:** Self-driving cars, drones, and robots will become increasingly common.\n",
       "2. **Personalized Medicine:** AI-powered systems will enable personalized treatment plans for patients.\n",
       "3. **Smart Cities:** AI-powered systems will optimize energy consumption, traffic flow, and public services.\n",
       "\n",
       "Overall, the future of AI is exciting and holds much potential for transforming various industries and aspects of our lives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c8548",
   "metadata": {},
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a1a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2015bc8",
   "metadata": {},
   "source": [
    "Lets wrap these functions and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4af01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5f2b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e5017f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Retrieval-Augmented Generation (RAG) is a novel approach to natural language processing (NLP) that combines the strengths of both retrieval and generation models. The goal of RAG is to leverage the capabilities of retrieval models, which excel at finding relevant information in large databases, with the power of generation models, which can create coherent and context-specific text.\n",
       "**Key Components:**\n",
       "1. **Retrieval Model:** A pre-trained language model (e.g., BERT or RoBERTa) that is fine-tuned on a specific task, such as question answering or text classification.\n",
       "2. **Generation Model:** A sequence-to-sequence model (e.g., transformer-based) that generates text based on the retrieved information.\n",
       "**How RAG Works:**\n",
       "1. The retrieval model is used to retrieve relevant documents or passages from a large database.\n",
       "2. The generated text is then fed into the generation model, which uses the retrieved information as input to generate coherent and context-specific text.\n",
       "3. The output of the generation model is refined through multiple iterations, with the retrieval model providing feedback on the relevance and accuracy of the generated text.\n",
       "**Advantages:**\n",
       "1. **Improved Accuracy:** RAG can achieve state-of-the-art performance on various NLP tasks, such as question answering and text classification.\n",
       "2. **Increased Efficiency:** By leveraging the strengths of both retrieval and generation models, RAG can reduce the computational resources required for training and inference.\n",
       "3. **Contextual Understanding:** RAG can provide a deeper understanding of the context in which the generated text is being used.\n",
       "**Applications:**\n",
       "1. **Question Answering Systems:** RAG can be used to build more accurate question answering systems that can retrieve relevant information from large databases.\n",
       "2. **Text Generation:** RAG can be applied to generate coherent and context-specific text for various applications, such as chatbots, content generation, and language translation.\n",
       "**Future Directions:**\n",
       "1. **Multitask Learning:** Exploring the use of multitask learning to fine-tune both retrieval and generation models simultaneously.\n",
       "2. **Explainability:** Developing techniques to provide insights into how RAG generates text and what information is being retrieved from the database.\n",
       "3. **Scalability:** Investigating ways to scale up RAG to accommodate large databases and complex NLP tasks.\n",
       "Overall, Retrieval-Augmented Generation (RAG) has shown great promise in improving the accuracy and efficiency of various NLP tasks. As research continues to advance, we can expect to see even more innovative applications of this technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0ed50",
   "metadata": {},
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec164cc",
   "metadata": {},
   "source": [
    "## LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda50dd",
   "metadata": {},
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6764b4",
   "metadata": {},
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58174402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_7648\\1437714523.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "f:\\github\\langChain-v0.3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "f:\\github\\langChain-v0.3\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USERAS\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "f:\\github\\langChain-v0.3\\.venv\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Use a local sentence-transformers model (no OPENAI_API_KEY needed)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d21de2",
   "metadata": {},
   "source": [
    "Here you can see the prompt does have three inputs, two for context and one for the question itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee17d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4519efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431cc06",
   "metadata": {},
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49680520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1230b97",
   "metadata": {},
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](../assets/lcel-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe4d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa40b2",
   "metadata": {},
   "source": [
    "We `invoke` it as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2a21b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model DeepSeek-V3 uses a Mixture of Experts (MoE) architecture.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026ce8b",
   "metadata": {},
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langChain-v0.3 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
