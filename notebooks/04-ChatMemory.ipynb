{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c893773",
   "metadata": {},
   "source": [
    "## Conversational Memory with Gemini API – LangChain Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4d695",
   "metadata": {},
   "source": [
    "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
    "\n",
    "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a8f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core\\\n",
    "  langchain-google-genai\\\n",
    "  langchain-community \\\n",
    "  langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e373ea6",
   "metadata": {},
   "source": [
    "## LangChain's Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc8196",
   "metadata": {},
   "source": [
    "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
    "\n",
    "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
    "\n",
    "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
    "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
    "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
    "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
    "\n",
    "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ff9e8",
   "metadata": {},
   "source": [
    "## Initialize our LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203251b",
   "metadata": {},
   "source": [
    "We start by initializing our LLM. We will use Gemini `gemma-3n-e4b-it` model, if you need an API key you can get one from [Google AI Studio](https://aistudio.google.com/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed19d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini initialized successfully with API key!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from constant import gemini_key  \n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_key\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "gemini_model = \"gemini-1.5-flash\"   \n",
    "\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=gemini_model, temperature=0.0)  \n",
    "creative_llm = ChatGoogleGenerativeAI(model=gemini_model, temperature=0.9)  # creative\n",
    "\n",
    "print(\"Gemini initialized successfully with API key!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24ac14",
   "metadata": {},
   "source": [
    "## 1. `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f219d",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
    "\n",
    "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21f5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_21100\\262359508.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b6b81",
   "metadata": {},
   "source": [
    "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
    "\n",
    "```\n",
    "User: Hi, my name is James\n",
    "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
    "User: I'm researching the different types of conversational memory.\n",
    "AI: That's interesting, what are some examples?\n",
    "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
    "AI: That's interesting, what's the difference?\n",
    "User: Buffer memory just stores the entire conversation, right?\n",
    "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
    "User: Buffer window memory stores the last k messages, dropping the rest.\n",
    "AI: Very cool!\n",
    "```\n",
    "\n",
    "We do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6188d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc8ae1",
   "metadata": {},
   "source": [
    "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f42822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0151a8",
   "metadata": {},
   "source": [
    "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55910842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac2a3b",
   "metadata": {},
   "source": [
    "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bdea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_21100\\839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0f5d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\" : \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e64e1a",
   "metadata": {},
   "source": [
    "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8bac0",
   "metadata": {},
   "source": [
    "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
    "\n",
    "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c4c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are helpfull assistant called zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c8908",
   "metadata": {},
   "source": [
    "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7505e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e085e77",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668ffae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "chat_map = {}\n",
    "def get_caht_history(session_id : str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d4d6f",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a49d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_caht_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dac9ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--cde7371e-db01-43d5-87d2-f1870181493f-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c48dac",
   "metadata": {},
   "source": [
    "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a724d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--499f4d3f-5317-474d-8297-bf3b08cf826c-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4c12e",
   "metadata": {},
   "source": [
    "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28397c51",
   "metadata": {},
   "source": [
    "## 2. `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b7652",
   "metadata": {},
   "source": [
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
    "\n",
    "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
    "\n",
    "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5010f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_21100\\3848892999.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k = 4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 4, return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fa2ba",
   "metadata": {},
   "source": [
    "We populate this memory using the same methods as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941222ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb751ad",
   "metadata": {},
   "source": [
    "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74cb1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6e650",
   "metadata": {},
   "source": [
    "\n",
    "Now let's see if our LLM remembers our name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e94f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"I do not know your name.  Our conversation so far has only focused on types of conversational memory; I haven't been given any information about your identity.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\" : \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ccb43",
   "metadata": {},
   "source": [
    "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
    "\n",
    "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a2e48",
   "metadata": {},
   "source": [
    "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5565358",
   "metadata": {},
   "source": [
    "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
    "\n",
    "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71fda461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1336035",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85832905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a387f",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48069a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--d9699ddf-e135-43c0-9395-5ada2585f339-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c82e3c",
   "metadata": {},
   "source": [
    "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c94c10e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf166c4",
   "metadata": {},
   "source": [
    "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16b70db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I do not know your name.  I have no memory of past conversations.  Every interaction with me starts fresh.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--10df5185-7637-4191-b9ee-3e3aa4f2fa7c-0', usage_metadata={'input_tokens': 50, 'output_tokens': 25, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089e19a",
   "metadata": {},
   "source": [
    "Now let's initialize a new session with `k=14`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "283e8850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--1b55e921-b92c-4606-bde3-ab76172052a4-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "744bb9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--1b55e921-b92c-4606-bde3-ab76172052a4-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a5af1",
   "metadata": {},
   "source": [
    "Now let's see if the LLM remembers our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f3ce934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--9d8bafa7-f647-4ca2-b631-fe062df236bf-0', usage_metadata={'input_tokens': 121, 'output_tokens': 6, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "577a5ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--1b55e921-b92c-4606-bde3-ab76172052a4-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--9d8bafa7-f647-4ca2-b631-fe062df236bf-0', usage_metadata={'input_tokens': 121, 'output_tokens': 6, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf002636",
   "metadata": {},
   "source": [
    "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080419f3",
   "metadata": {},
   "source": [
    "## 3. `ConversationSummaryMemory`\n",
    "\n",
    "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
    "\n",
    "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f962adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_21100\\988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2a303",
   "metadata": {},
   "source": [
    "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
    "\n",
    "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3b8f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca8018",
   "metadata": {},
   "source": [
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dc78328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI. The AI responds, explaining that it is a large language model trained by Google, lacking a personal identity, and offering its services to answer questions, translate languages, and create content.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI, which identifies itself as a Google-trained large language model without a personal identity, offering question answering, translation, and content creation services.  James then inquires about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions.  It categorizes approaches as short-term memory (STM), holding immediate information, and long-term memory (LTM), storing information across multiple turns, implemented through retrieval-based or generative models.  Further distinctions are made between episodic memory (remembering specific events) and semantic memory (remembering general facts), and between explicit memory (directly stored and retrieved) and implicit memory (embedded within the model's internal state). The AI concludes that the type of conversational memory used depends on the application's complexity, with sophisticated systems often employing a hybrid approach, and that research in this area is ongoing.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI, a Google-trained large language model offering question answering, translation, and content creation, and inquires about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions, categorizing approaches as short-term memory (STM) and long-term memory (LTM), with further distinctions between episodic and semantic, and explicit and implicit memory.  The AI notes that the type of memory used depends on application complexity, with sophisticated systems often using a hybrid approach.  James then mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains these as specific STM implementations, describing `ConversationBufferMemory` as a simple buffer storing a limited number of recent conversation turns, and `ConversationBufferWindowMemory` as a similar system using a sliding window to maintain access to a fixed number of recent turns.  Both are retrieval-based, prioritizing efficiency over deep semantic understanding, with the choice between them depending on the balance between memory capacity and computational efficiency.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI, a Google-trained large language model, and asks about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions, differentiating between short-term memory (STM) and long-term memory (LTM), with further distinctions between episodic and semantic, and explicit and implicit memory. The AI notes that the type of memory used depends on application complexity, with sophisticated systems often using a hybrid approach. James mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI clarifies as specific STM implementations.  `ConversationBufferMemory` is described as a simple buffer storing a limited number of recent conversation turns using a FIFO (First-In, First-Out) approach, while `ConversationBufferWindowMemory` uses a sliding window to maintain access to a fixed number of recent turns. Both are retrieval-based, prioritizing efficiency over deep semantic understanding, with the choice between them depending on the balance between memory capacity and computational efficiency.  The AI corrects the misconception that buffer memory stores the entire conversation, emphasizing that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` only store a *limited* portion of the recent conversation, differing in how that limited portion is managed.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': \"James introduces himself to the AI, a Google-trained large language model, and asks about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions, differentiating between short-term memory (STM) and long-term memory (LTM), with further distinctions between episodic and semantic, and explicit and implicit memory. The AI notes that the type of memory used depends on application complexity, with sophisticated systems often using a hybrid approach. James mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI clarifies as specific STM implementations.  `ConversationBufferMemory` is described as a simple buffer storing a limited number of recent conversation turns using a FIFO (First-In, First-Out) approach, while `ConversationBufferWindowMemory` uses a sliding window to maintain access to a fixed number of recent turns. Both are retrieval-based, prioritizing efficiency over deep semantic understanding, with the choice between them depending on the balance between memory capacity and computational efficiency.  The AI corrects the misconception that buffer memory stores the entire conversation, emphasizing that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` only store a *limited* portion of the recent conversation, differing in how that limited portion is managed.\",\n",
       " 'response': \"That's correct.  `ConversationBufferWindowMemory`, as a sliding window implementation, maintains a fixed-size buffer containing the *k* most recent messages.  When a new message arrives, the oldest message (the one that's been in the buffer the longest) is dropped to make room.  This ensures that the memory always contains the last *k* messages, providing a recent history of the conversation.  The value of *k* is a parameter that determines the size of the window and, consequently, the amount of conversational history retained.  Larger values of *k* provide more context but increase memory consumption and computational overhead.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd7c86",
   "metadata": {},
   "source": [
    "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54e4d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI, a Google-trained large language model, and asks about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions, differentiating between short-term memory (STM) and long-term memory (LTM), with further distinctions between episodic and semantic, and explicit and implicit memory. The AI notes that the type of memory used depends on application complexity, with sophisticated systems often using a hybrid approach. James mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI clarifies as specific STM implementations.  `ConversationBufferMemory` is described as a simple buffer storing a limited number of recent conversation turns using a FIFO (First-In, First-Out) approach, while `ConversationBufferWindowMemory` uses a sliding window to maintain access to a fixed number of recent turns. Both are retrieval-based, prioritizing efficiency over deep semantic understanding, with the choice between them depending on the balance between memory capacity and computational efficiency.  The AI corrects the misconception that buffer memory stores the entire conversation, emphasizing that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` only store a *limited* portion of the recent conversation, differing in how that limited portion is managed.  The AI confirms that `ConversationBufferWindowMemory` stores the last *k* messages, dropping older messages when a new one arrives, where *k* is a configurable parameter determining the window size, balancing context retention with memory usage and computational cost.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': \"James introduces himself to the AI, a Google-trained large language model, and asks about conversational memory in AI. The AI explains that this refers to the system's ability to remember and utilize information from previous interactions, differentiating between short-term memory (STM) and long-term memory (LTM), with further distinctions between episodic and semantic, and explicit and implicit memory. The AI notes that the type of memory used depends on application complexity, with sophisticated systems often using a hybrid approach. James mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI clarifies as specific STM implementations.  `ConversationBufferMemory` is described as a simple buffer storing a limited number of recent conversation turns using a FIFO (First-In, First-Out) approach, while `ConversationBufferWindowMemory` uses a sliding window to maintain access to a fixed number of recent turns. Both are retrieval-based, prioritizing efficiency over deep semantic understanding, with the choice between them depending on the balance between memory capacity and computational efficiency.  The AI corrects the misconception that buffer memory stores the entire conversation, emphasizing that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` only store a *limited* portion of the recent conversation, differing in how that limited portion is managed.  The AI confirms that `ConversationBufferWindowMemory` stores the last *k* messages, dropping older messages when a new one arrives, where *k* is a configurable parameter determining the window size, balancing context retention with memory usage and computational cost.\",\n",
       " 'response': 'Your name is James.  We established that at the beginning of our conversation.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276af5e",
   "metadata": {},
   "source": [
    "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29864092",
   "metadata": {},
   "source": [
    "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7002a",
   "metadata": {},
   "source": [
    "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a4a73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI\n",
    "\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
    "        super().__init__(llm=llm)\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "            \"\"\"Add messages to the history, removing any messages beyond\n",
    "            the last `k` messages.\n",
    "            \"\"\"\n",
    "            self.messages.extend(messages)\n",
    "\n",
    "            summary_prompt = ChatPromptTemplate.from_messages([\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing summary:\\n{existing_summary}\\n\\nNew messages:\\n{messages}\"\n",
    "                )\n",
    "            ])\n",
    "            # format the messages and invoke the LLM\n",
    "            new_summary = self.llm.invoke(\n",
    "                summary_prompt.format_messages(\n",
    "                    existing_summary=self.messages[0].content if self.messages else \"\",\n",
    "                    messages=\"\\n\".join([msg.content for msg in messages])\n",
    "                )\n",
    "            )\n",
    "            # replace the existing history with a single system summary message\n",
    "            self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0574d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e202a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"Session ID for chat history\",\n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatGoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"Gemini LLM for summarization\",\n",
    "            default=llm\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f224a",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17da4abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--e3bffd56-0729-401d-8f5c-21447341e580-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4dff0",
   "metadata": {},
   "source": [
    "Let's see what summary was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f69cf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='James introduced himself.  A greeting was returned and an offer of assistance was made.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5a6bb",
   "metadata": {},
   "source": [
    "Let's continue the conversation and see if the summary is updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc7ddad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='James introduced himself and requested information on conversational memory.  The response provided a detailed categorization of conversational memory from three perspectives: scope (short-term, intermediate-term, and long-term), type of information (factual, contextual, emotional, and user-specific), and implementation techniques (rule-based, statistical, knowledge graph-based, and embedding-based).  The response concluded by prompting James to specify his area of interest for more focused information.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911da6a6",
   "metadata": {},
   "source": [
    "So far so good! Let's continue with a few more messages before returning to the name question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "501e6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa878235",
   "metadata": {},
   "source": [
    "Let's see the latest summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba8f27b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='James is exploring short-term conversational memory implementations, specifically `ConversationBufferMemory` and `ConversationBufferWindowMemory`, for managing conversation history in lengthy interactions.  `ConversationBufferMemory` uses a simple fixed-size FIFO queue, discarding the oldest messages when full.  `ConversationBufferWindowMemory`, however, employs a weighting system to prioritize important conversation turns.  Unlike `ConversationBufferMemory`, it doesn\\'t simply drop the oldest messages; instead, it dynamically adjusts its retained \"window\" of messages based on relevance, discarding less important messages regardless of their recency.  Therefore, it maintains a prioritized selection of the most relevant messages, not just the last *k* messages as previously inaccurately stated.  To provide tailored advice, further clarification is needed on James\\'s specific use case, performance requirements, information retention needs, and programming environment.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ed561",
   "metadata": {},
   "source": [
    "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed899d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--5746b452-7c85-426f-a16b-886ecb991a1c-0', usage_metadata={'input_tokens': 186, 'output_tokens': 6, 'total_tokens': 192, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deef16",
   "metadata": {},
   "source": [
    "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18259e4",
   "metadata": {},
   "source": [
    "## 4. `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b9303",
   "metadata": {},
   "source": [
    "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
    "\n",
    "\n",
    "```\n",
    "# ~~ a summary of previous interactions\n",
    "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
    "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
    "examples.\n",
    "# ~~ the most recent messages\n",
    "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
    "AI: That's interesting, what's the difference?\n",
    "Human: Buffer memory just stores the entire conversation\n",
    "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
    "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
    "AI: Very cool!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7931a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_21100\\569741439.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369449ca",
   "metadata": {},
   "source": [
    "As before, we set up the deprecated memory type using the `ConversationChain` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bb4df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8dee3",
   "metadata": {},
   "source": [
    "First invoke with a single message:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc5d4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is James',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hi James! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine that can also hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Hi James! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine that can also hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"Hi, my name is James\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c043414",
   "metadata": {},
   "source": [
    "Looks good so far, let's continue with a few more messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "197f2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi James! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced, talkative search engine that can also hold a conversation.  So, what can I do for you today?  Are you interested in learning something new, writing a story, or maybe just chatting?', additional_kwargs={}, response_metadata={})]\n",
      "Human: I'm researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"James initiates a conversation with the AI, identifying himself and stating his research interest in conversational memory. The AI introduces itself as a large language model trained by Google, lacking personal identity or feelings.  It then explains several key types of conversational memory in AI: short-term memory (STM), which only remembers recent turns; long-term memory (LTM), which can be retrieval-based (using databases) or generative (integrating history into the model); episodic memory, which remembers specific events; and hybrid approaches combining different types.  The AI highlights the challenges and ongoing development in this field, offering to delve deeper into specific areas based on James's interest.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"James, researching conversational memory, initiates a conversation with a Google-trained large language model. The AI, lacking personal identity, explains various types of conversational memory: short-term (STM), long-term (LTM – retrieval-based and generative), and episodic memory, along with hybrid approaches.  It then elaborates on specific STM implementations, `ConversationBufferMemory` (a fixed-size FIFO buffer) and `ConversationBufferWindowMemory` (a sliding window prioritizing relevant turns).  The AI highlights the differences in buffer management, implementation details (lists, arrays, or more complex structures), and the trade-offs between context retention and computational efficiency.  Both are suitable for real-time applications needing immediate context but lack the capacity for extensive historical recall. The AI concludes by offering to explore specific aspects of these memory types further based on James's interest.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"James, researching conversational memory, initiates a conversation with a Google-trained large language model. The AI, lacking personal identity, explains various types of conversational memory: short-term (STM), long-term (LTM – retrieval-based and generative), and episodic memory, along with hybrid approaches. It then elaborates on specific STM implementations, `ConversationBufferMemory` (a fixed-size FIFO buffer) and `ConversationBufferWindowMemory` (a sliding window prioritizing relevant turns).  The AI highlights the differences in buffer management, implementation details (lists, arrays, or more complex structures), and the trade-offs between context retention and computational efficiency.  Both are suitable for real-time applications needing immediate context but lack the capacity for extensive historical recall.  The AI clarifies that these buffer memories do not store the *entire* conversation, but rather a limited portion. `ConversationBufferMemory`, a FIFO buffer, acts like a queue with a fixed size, discarding older turns as new ones are added.  `ConversationBufferWindowMemory`, conversely, uses a sliding window, prioritizing relevant turns based on techniques like keyword matching or semantic similarity, potentially using more complex data structures than simple lists or arrays.  The AI emphasizes that both methods manage limited recent conversational history, optimized for speed and efficiency in real-time applications, at the cost of losing older conversation details, differing primarily in their management of that limited history: FIFO for `ConversationBufferMemory` and a relevance-based sliding window for `ConversationBufferWindowMemory`. The AI concludes by offering to explore specific aspects of these memory types further based on James's interest.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9d8de",
   "metadata": {},
   "source": [
    "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb204f2",
   "metadata": {},
   "source": [
    "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f51d55",
   "metadata": {},
   "source": [
    "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
    "\n",
    "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74346a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI\n",
    "    k: int\n",
    "\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        existing_summary = None\n",
    "        old_messages = None\n",
    "\n",
    "        if self.messages and isinstance(self.messages[0], SystemMessage):\n",
    "            existing_summary = self.messages.pop(0)\n",
    "\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        if len(self.messages) > self.k:\n",
    "            old_messages = self.messages[:len(self.messages) - self.k]\n",
    "            self.messages = self.messages[-self.k:]\n",
    "\n",
    "        if old_messages is None:\n",
    "            return\n",
    "\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, generate a new summary. Preserve relevant details.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing summary:\\n{existing_summary}\\n\\nNew messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary.content if existing_summary else \"\",\n",
    "                old_messages=\"\\n\".join([msg.content for msg in old_messages])\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24357be1",
   "metadata": {},
   "source": [
    "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c59f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2fbc5",
   "metadata": {},
   "source": [
    "Setup our pipeline with new configurable fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b04d0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"Session ID for chat history\",\n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatGoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"Gemini LLM for summarization\",\n",
    "            default=llm\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"Number of messages to keep in buffer\",\n",
    "            default=4\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032a9a8",
   "metadata": {},
   "source": [
    "Finally, we invoke our runnable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d418fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--70d0adaf-bc01-4cc0-ae5a-b5cea052698c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb142552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 11\n",
      "}\n",
      "].\n",
      "Error in RootListenersTracer.on_chain_end callback: ResourceExhausted('You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.')\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b725aac",
   "metadata": {},
   "source": [
    "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langChain-v0.3 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
